# reduce-llm

## Helper Scripts

- `chunk_file.py` splits the 16GB wikipedia dump file into smaller files.

  Usage: `python3 chunk_file.py [enwiki-latest-pages-articles.txt] 1000000 -o [output-directory]`
