# reduce-llm

## Helper Scripts

- `chunk_file.py` splits the 16GB wikipedia dump file into smaller files.
